{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Individual Machine Learning - Blas Collias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "En un principio, nos encontramos ante un problema de clasificación, tenemos que predecir ante nuevos registros de pacientes si su estadía va a ser larga o corta(Clasificación binaria). Estadia corta: cuando dura 8 o menos días, y larga se considera a las estadias que duran mas de 8 dias.\n",
    "\n",
    "El objetivo de esta clasificacion es que en base a los registros historicos, podamos administrar la demanda de camas segun la condicion en la que llegan los pacientes recien ingresados. Para qué? Para mejorar la eficiencia en la prestacion de servicios de salud, disminuir los costos y la saturacion de hospitales, evitar enfermedades intrahospitalarias, que son todos efectos negativos derivados de las estancias hospitalarias prolongadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA y Preprocesamiento de datos\n",
    "\n",
    "En esta parte, trabajamos en el notebook llamado 'Proyecto.ipynb', en el cual están detallados y en orden cada uno de los pasos del proceso.\n",
    "En primer lugar, ingestamos la data y obtuvimos un pantallazo(exploración de los datos) para empezar a entrar en confianza con la misma. Pudimos ver el tamaño de los datos(filas y columnas), en detalle cada una de las variables y el tipo de datos de cada columna, y un resumen estadístico de las variables numericas.\n",
    "\n",
    "Seguimos con  la búsqueda y el tratamiento de valores nulos(no había), de valores duplicados(tampoco había) y de los valores outliers que los detectamos a simple vista con graficos como el de boxplot(de caja y bigotes/brazos).\n",
    "\n",
    "Escalamos(con el metodo StandardScaler) algunas variables y normalizamos las categóricas con métodos como labelEncoder y OrdinalEncoder según si estas eran nominales u ordinales con el fin de obtener un dataframe con exclusivamente números. Tambien renombramos algunas columnas para facilitar el manejo de las variables.\n",
    "\n",
    "Visualizacion de datos:\n",
    "Mediante graficos countplot de la librería seaborn, vimos la distribucion de algunas variables respecto a la variable target y sacamos buenas conclusiones como por ejemplo: en el area/departamento de anestesia y cirugía solo se presentaban casos de estadias largas, había 2 de los doctores(Simon y Isaac) que a todos los pacientes que atendian derivaban en una estadia larga. Tambien, que los de edad>50 años terminaban siendo siempre casos de estadia larga y otras conclusiones mas que nos ayudaron o guiaron para ver qué variables podían pesar mas o tener mas importancia a la hora de realizar un modelo de machine learning.\n",
    "\n",
    "Utilizamos otros gráficos como Pairplot y la matriz de Correlación que tambien te permiten observar a simple vista, algunas relaciones y la fuerza de esas relaciones entre variables.\n",
    "\n",
    "Finalmente decidí utilizar un dataframe con estas columnas:(['habitaciones_disponibles', 'area', 'doctor', 'personal_disponibles', 'visitas','seguro', 'deposito', 'target', 'gravedad_enc', 'edad_enc']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Ante el problema de clasificación, decidimos utlizar algunos modelos los cuale fuimos probando y observando como actuaban al predecir. En un principio, empezamos con modelos mas simples(arbol de decision, regresion logistica) y fuimos avanzando con modelos mas complejos con los cuales nos quedamos.\n",
    "\n",
    "1- Modelo de ensamble Bagging para el arbol de decision : decidimos utilizar este porque es un conjunto de modelos de ML, que se combinan para obtener una unica prediccion y su principal ventaja es que al ser diferentes modelos, los errores tienden a compensarse y obteniendo asi un mejor error de generalizacion.\n",
    "\n",
    "2- Random Forest(DEFINITIVO): este fue el definitivo y el que subí al dashboard. Lo elegimos porque una de sus ventajas es que maneja bien hasta miles de variables e identifica las mas importantes, siendo un metodo de reduccion de dimensionalidad.\n",
    "\n",
    "Le pasamos como parametro: la cantidad de arboles que va a tener el bosque y elegí 100 que es un buen valor por defecto(n_estimators), n_jobs = -1 que indica que va a utilizar tantos cores como tiene la máquina, random_state=42 que es el tipo de aleatoriedad, max_features='sqrt' que se refiere a tomar las n_features que tengas. Además podiamos jugar con otros parametros como max_depth, min_samples_split y min_samples_leaf.\n",
    "\n",
    "Separamos nuestra data con train_test_split pero podriamos haberlo hecho tambien con otros metodos como cross_validation o k-folds por ejemplo.\n",
    "\n",
    "El score del bosque(porque son varios arboles) sobre la importancia de las variables, es un promedio que se normaliza a partir de la desviación estandar. En mi caso decidí mostrarlos tambien en un grafico de barras el cual las mostraba ordenadas.\n",
    "\n",
    "Las predicciones las pasamos a un dataframe y las guardamos en un csv para subirlo al dashboard.\n",
    "\n",
    "3- Por ultimo, utilizamos un Arbol de decision con max_depth=10 y tambien aplicamos el GridSearch para ajustar parámetros.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63035aa01ef6bd32010057b8dafdd246fd47813ff2d54e3fd56d0bdae14f37d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
